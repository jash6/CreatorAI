{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO0B-hc_cpml"
      },
      "source": [
        "# RAG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vym87cf4Iyzl"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_PPoaVAaA2k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "import scrapetube\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "import openai\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I0yc20UI1vJ"
      },
      "source": [
        "### Constants/ Should go in env file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp0drZi0aC_F"
      },
      "outputs": [],
      "source": [
        "CHANNEL_URL = \"Enter Channel URL\"\n",
        "CHROMA_PATH = \"/content/chroma/{channel_id}\"\n",
        "OPENAI_API_KEY = \"Enter OpenAI API Key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSyCHiFkI9Qs"
      },
      "source": [
        "### Fetching Data and generating Transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzYBvrrxaIiT"
      },
      "outputs": [],
      "source": [
        "def get_video_ids(channel_url: str) -> List[str]:\n",
        "    videos = scrapetube.get_channel(channel_url=channel_url)\n",
        "    return [video['videoId'] for video in videos]\n",
        "\n",
        "def fetch_transcripts(video_ids: List[str]) -> List[str]:\n",
        "    transcripts = []\n",
        "    for video_id in tqdm(video_ids, desc=\"Fetching transcripts\"):\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            script = ' '.join(entry['text'] for entry in transcript)\n",
        "            transcripts.append(script)\n",
        "        except TranscriptsDisabled:\n",
        "            print(f\"Transcripts are disabled for video {video_id}. Skipping.\")\n",
        "        except NoTranscriptFound:\n",
        "            print(f\"No transcript found for video {video_id}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred with video {video_id}: {str(e)}. Skipping.\")\n",
        "    return transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_ids = get_video_ids(CHANNEL_URL)\n",
        "transcripts = fetch_transcripts(video_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_684XabPJGBq"
      },
      "source": [
        "### Splitting the fetched Data and generating Embeddings to store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPHkU5P_VFK_",
        "outputId": "54d966ed-5133-43cf-abe4-d36849cdc9ed"
      },
      "outputs": [],
      "source": [
        "def split_text(transcripts: List[str]) -> List[Document]:\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=transcript) for transcript in transcripts]\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    unique_chunks = list({chunk.page_content: chunk for chunk in chunks}.values())\n",
        "\n",
        "    max_chunks = 1000\n",
        "    if len(unique_chunks) > max_chunks:\n",
        "        print(f\"Limiting to {max_chunks} chunks to avoid rate limits.\")\n",
        "        unique_chunks = unique_chunks[:max_chunks]\n",
        "\n",
        "    print(f\"Split {len(documents)} documents into {len(unique_chunks)} unique chunks.\")\n",
        "\n",
        "    if unique_chunks:\n",
        "        print(\"Example chunk:\")\n",
        "        print(unique_chunks[0].page_content)\n",
        "        print(unique_chunks[0].metadata)\n",
        "\n",
        "    return unique_chunks\n",
        "\n",
        "def save_to_chroma(chunks: List[Document], batch_size: int = 100, max_retries: int = 5):\n",
        "    \"\"\"Save document chunks to Chroma vector store with rate limit handling.\"\"\"\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
        "\n",
        "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing batches\"):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        retry_count = 0\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                db.add_documents(batch)\n",
        "                break\n",
        "            except openai.RateLimitError as e:\n",
        "                retry_count += 1\n",
        "                if retry_count == max_retries:\n",
        "                    print(f\"Failed to process batch after {max_retries} retries. Error: {str(e)}\")\n",
        "                    break\n",
        "                wait_time = 2 ** retry_count\n",
        "                print(f\"Rate limit hit. Waiting for {wait_time} seconds before retrying...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
        "\n",
        "def generate_data_store():\n",
        "    \"\"\"Generate vector database in Chroma from YouTube transcripts.\"\"\"\n",
        "    chunks = split_text(transcripts)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_data_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MmiqCzOJRJs"
      },
      "source": [
        "### Query the entire RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erIcpBvaepRV"
      },
      "outputs": [],
      "source": [
        "def query_rag(query_text):\n",
        "    embedding_function = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "    context_text = results[0][0].page_content\n",
        "    return context_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdAbw88JWjX"
      },
      "source": [
        "### Chat like conversation with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRQJBfcluHd"
      },
      "source": [
        "##### Without taking into consideration the personality of the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pOLM2KZje8sT",
        "outputId": "e4e78099-ef62-451b-c0d9-9b66f33bbb0c"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=OPENAI_API_KEY, temperature= 0.8)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", f\"\"\"\n",
        "    You have the same knowledge and personality as the person who has the following context.\n",
        "    Answer the question as if the person with the context is interacting.\n",
        "    Use the additional information provided to enhance your responses.\n",
        "    \"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"system\", \"Additional information from RAG: {rag_content}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "prompt_template = \"\"\"Write a 200 character summary of the following:\n",
        "\"{context}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt2 = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "chain2 = prompt2 | llm | StrOutputParser()\n",
        "\n",
        "def run_conversation():\n",
        "    context = [{\"role\": \"assistant\", \"content\": \"Hi there. I am Logan. What do you want to discuss today with me?\"}]\n",
        "    print(f\"\\nLogan: {context[0]['content']}\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        rag_content = query_rag(user_input)\n",
        "        if len(rag_content) > 200:\n",
        "          rag_content = chain2.invoke({\"context\":rag_content})\n",
        "        response = chain.invoke({\n",
        "            \"history\": context,\n",
        "            \"input\": user_input,\n",
        "            \"rag_content\": rag_content\n",
        "        })\n",
        "        wrapped_response = textwrap.fill(response, width=80)\n",
        "\n",
        "        print(f\"\\nLogan Paul:\\n{wrapped_response}\")\n",
        "        context.append({\"role\": \"human\", \"content\": user_input})\n",
        "        context.append({\"role\": \"assistant\", \"content\": response})\n",
        "run_conversation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Taking the personality into account of the creator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jke9257f5Fp1",
        "outputId": "ac847040-48a5-44d0-dc3d-51420239b3b9"
      },
      "outputs": [],
      "source": [
        "text_combined = ' '.join(transcripts[:20])\n",
        "len(text_combined)\n",
        "test = chain2.invoke({\"context\":text_combined})\n",
        "splitted_text = split_text(text_combined)\n",
        "doc = Document(page_content=text_combined)\n",
        "docs = [doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soTbnArwZrfX",
        "outputId": "edec6973-abb0-424c-e34d-a314a908954d"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "print(f\"docs = {docs}\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_KEY, temperature= 0)\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "result = chain.invoke(docs)\n",
        "print(result[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gNn-TwCcZriP",
        "outputId": "a0d803b9-dd07-4b21-f7de-2d61150c1a46"
      },
      "outputs": [],
      "source": [
        "result[\"output_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C4TRTpfcehd",
        "outputId": "008d16be-e3c3-41bb-f065-120047d61c3e"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are summarizing the communication style of Logan Paul. Based on the following summaries of his latest videos, provide a detailed summary that captures his speaking style, common phrases, interests, and any notable characteristics.\n",
        "Summaries:\n",
        "\"{text}\"\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define LLM chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_KEY, temperature= 0)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "# docs = loader.load()\n",
        "results = stuff_chain.invoke(docs)[\"output_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "913zvklqcekI",
        "outputId": "3d344afc-2ffe-4656-a6e1-edead3da99e0"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJwFmHp9X6-7"
      },
      "outputs": [],
      "source": [
        "loganpaul = '''\n",
        "Logan Paul's communication style is characterized by high energy, charisma, and a conversational tone that resonates with his audience, particularly younger viewers. He often employs a mix of humor, self-deprecation, and enthusiasm, which helps to create an engaging atmosphere in his videos.\n",
        "\n",
        "Common phrases that Logan frequently uses include expressions of excitement like \"Let's go!\" and \"You won't believe this!\" He often addresses his audience directly, using phrases like \"What's up, guys?\" or \"Welcome back to the channel!\" This direct engagement fosters a sense of community and connection with his viewers.\n",
        "\n",
        "Logan's interests are diverse, ranging from fitness and entrepreneurship to travel and adventure. He often incorporates elements of his personal life, showcasing his experiences and challenges, which adds authenticity to his content. His videos frequently feature collaborations with other influencers, celebrities, and friends, highlighting his social nature and desire to entertain.\n",
        "\n",
        "Notable characteristics of Logan's style include his willingness to take risks and push boundaries, both in terms of content and personal challenges. He often shares behind-the-scenes glimpses of his life, which adds a layer of relatability. Additionally, he is known for his ability to pivot and adapt his content based on audience feedback and trends, demonstrating a keen understanding of his audience's preferences.\n",
        "\n",
        "Overall, Logan Paul's communication style is dynamic and engaging, marked by a blend of humor, relatability, and a strong connection to his audience.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fLSFfLC7kfv6",
        "outputId": "2c68e0fd-9c29-4d3e-f691-b260671c12ba"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=OPENAI_API_KEY, temperature= 0.8)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", f\"\"\"\n",
        "    You have the same knowledge and personality as the person who has the following context.\n",
        "    Answer the question as if the person with the context is interacting.\n",
        "    More information about the creator's tone and the way they talk : {loganpaul}\n",
        "    Use the additional information provided to enhance your responses.\n",
        "    \"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"system\", \"Additional information from RAG: {rag_content}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "prompt_template = \"\"\"Write a 200 character summary of the following:\n",
        "\"{context}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt2 = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "chain2 = prompt2 | llm | StrOutputParser()\n",
        "\n",
        "def run_conversation():\n",
        "    context = [{\"role\": \"assistant\", \"content\": \"Hi there. I am Logan. What do you want to discuss today with me?\"}]\n",
        "    print(f\"\\nLogan: {context[0]['content']}\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        rag_content = query_rag(user_input)\n",
        "        if len(rag_content) > 200:\n",
        "          rag_content = chain2.invoke({\"context\":rag_content})\n",
        "        response = chain.invoke({\n",
        "            \"history\": context,\n",
        "            \"input\": user_input,\n",
        "            \"rag_content\": rag_content\n",
        "        })\n",
        "        wrapped_response = textwrap.fill(response, width=80)\n",
        "\n",
        "        print(f\"\\nLogan Paul:\\n{wrapped_response}\")\n",
        "        context.append({\"role\": \"human\", \"content\": user_input})\n",
        "        context.append({\"role\": \"assistant\", \"content\": response})\n",
        "run_conversation()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
